\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{MLCV Project 2017} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
% \setcounter{page}{4321}

\begin{document}

%%%%%%%%% TITLE
\title{Cutting Constraints on Conservation Tracking\\
  \large{Project Report MLCV Summer 2017}
}

\author{Kodai Matsuoka\\
{\tt\small kodaig06@gmail.com}
\and
Yuyan Li\\
{\tt\small yuyan.li@gmx.net}
\and
Jui-Hung Yuan\\
{\tt\small j.yuan@stud.uni-heidelberg.de}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

To understand the complex biological functions of living organisms, many experiments require monitoring of stem cells or bacteria over several generations under different conditions. However, those time-lapse experiments generate large amount of data, which human observers could hardly analyze without bias. Thus, automatated systems for cell tracking are necessary for those studies.

The analysis of the time-lapse microscopic results usually requires not only the tracking of position and locomotion of individual cells, but also the reconstruction of their full lineage. In comparison to pedestrian tracking, the cell tracking task is more challenging due to the constant change of the cellular texture and morphology throughout the cell cycle, the high density of cells with uncertain movement as well as the division events which is not included in other multi-object tracking tasks.

To tackle the cell tracking task, a two-step pipeline consisting of a segmentation/detection phase and an assignment/tracking phase is commonly used. In the first phase, the raw input images are segmented into foreground and background. Those segmentations and the raw data are then fed into classifiers that generate the corresponding detection and division hypotheses. Using the outputs of the classifiers as potentials, a graphical model is built for all possible assignments of  detection hypotheses between time frames in the second phase. Such tracking approaches are known as \textit{Tracking-by-assignment} methods, which assume that the previous extracted set of detection hypotheses are over-complete and the constructed model thus describe all tracked targets. A globally consistent tracking solution is then reached via various optimization strategies.

The drawback of such method is that the errors in the first stage would propagate to, and warp, the tracking result. Such errors would occur where a cluster of objects is incorrectly represented by a single segment, termed as \textit{mergers} in this paper. To correct these over- and undersegmentation errors, the Conservation Tracking model which explicitly include the global consistency constraints was developed and was shown to outperform other tracking methods. From previous works, the Conservation Tracking model could be further reformulated into a constrained network flow problem which led to a tight LP relaxation and thus could be solved faster. Even without the constraints for divisions and mergers, the LP relaxation of the model that accounts for flow conservation would still yield integral solutions.

In this paper, we hence developed a iterative method that solve the problem with only the violated constraints identified in each iteration. (a bit more blablabla)


\section{Constrained Network Flow Reformulation of Consveration Tracking}


\subsection{Conservation Tracking Model}

\begin{figure}[t]
\begin{center}
\centering\includegraphics[width=0.8\linewidth]{model.jpeg}
\end{center}
   \caption{Factor graph for one detection with two incoming and two outgoing transition candidates. The circular nodes represent random variables, the non-black boxes describe factors that are dependent on the connected variables, and filled black boxes represent constraints. The purple detection node $i$ is seperated into a disappearance variable $V_i$ and an appearance variable $A_i$. The binary variable $D_i$ indicates whether object $i$ is about to divide. The transition variables $T \in \{0,...,m\}$ indicates the number of objects that are linked between two detection hypotheses.}
\label{fig:ctmodel}
\end{figure}

The Conservation Tracking model is presented in factor graphs as in Fig. \ref{fig:ctmodel}. The model contains three types of variables: \textit{Detection} variables $X_i \in \mathcal{X}$ (including the appearance $A_i \in \mathcal{A}$ and disappearance variables $V_i \in \mathcal{V}$) for each connected object from the segmented image, binary \textit{dividing} variables $D_i \in \mathcal{D}$ indicating whether an object is about to divide, and \textit{transition} variables $T_i \in \mathcal{T}$ that represent connections between the detections in two neighboring time frames. Noteworthy is that a division is allowed if and only if the corresponding detection contains one object. The probabilities of transition are modeled using the center of mass distance between detections, while the probabilities of appearance and disappearance are constant throughout time but linearly decrease to the edge of each image.

Let $\mathcal{Y}$ be the complete set of the configurations of all variables $\mathcal{V}\cup\mathcal{A}\cup\mathcal{T}\cup\mathcal{D}$, the approximate maximum a-posteriori (MAP) solution of the factor graph can be found by minimizing the energy
\scriptsize
\begin{equation*}
\begin{split}
 y^* &= \operatorname*{arg\,max}_{y\in\mathcal{Y}} E(y) \\ 
     &= \operatorname*{arg\,max}_{y\in\mathcal{Y}} \sum\limits_{V\in\mathcal{V}}\sum\limits_{A\in\mathcal{A}} E_x(y_V, y_A) + \sum\limits_{T\in\mathcal{T}} E_T(y_T) + \sum\limits_{D\in\mathcal{D}} E_D(y_D)
\end{split}
\end{equation*}
\normalsize
subject to constraints for flow conservation, division and mergers which will be explained in \ref{ILP formulation} in detail. This graphical model can be reformulated into a network flow and solved as an integer linear programming (ILP) problem.

\subsection{ILP for Network Flow}
\label{ILP formulation}
Linear Programming (LP) is a method to minimize (maximize) linear objective function, subject to linear inequality. Its formulation is as below.
\[
min_{x} c^{T}x
\]
\[
s.t. Ax \leq b
\]
The objective function \(c^{T}x\) linearly depends on the vector \(x \in R^{n}\) which represents n variables in question. Constraint \(Ax \leq b\) defines n hyperplanes that disallow x to lie in the back side. The area where x satisfies all the constraints is called polytope. It is proved that the optimal solution lies on a vertex of the polytope. For this reason, LP is solved in polynomial time by searching along vertices on the polytope.

When x takes only integer values (i.e. \(x \in Z^{n}\), the problem is called Integer Linear Programs (ILP). ILP is NP hard and cannot be solved in polynomial time in general. 

A common approach to solve ILPs is to ignore the integrarity constraints and to solve this relaxed LP. Since this "relaxed" problem is LP, we can solve it in polynomial time. From the solution, we gain the information about original ILP problem.
If the solution of relaxed LP is integtal - and hence coinsides with the solution of original ILP - , the relaxation is said to be "tight" around the optimum.

Our Network flow ILP looks like this:

\begin{equation*}
\begin{split}
 y^* &= \operatorname*{arg\,max}_{y\in\mathcal{Y}} E(y) \\ 
     &= \operatorname*{arg\,max}_{y\in\mathcal{Y}} \sum\limits_{X\in\mathcal{X}} E_X(y_X) + \sum\limits_{T\in\mathcal{T}} E_T(y_T) + \sum\limits_{D\in\mathcal{D}} E_D(y_D) \\
     &= \operatorname*{arg\,max}_{y\in\mathcal{Y}} \sum\limits_{X\in\mathcal{X}} \sum\limits_{k\in\mathcal{L(X)}} \theta_X(k) 1[y_X = k] + \sum\limits_{T\in\mathcal{T}} \sum\limits_{k\in\mathcal{L(T)}} 1[y_T = k] + \sum\limits_{D\in\mathcal{D}} \sum\limits_{k\in\mathcal{L(D)}} 1[y_D = k] 
\end{split}
\end{equation*}

subject to

Flow conservarion:

Division:

Merger:

\subsection{Loosening Constraints}

As stated before, we know that LP relaxation on the network flow without \textit{divisions} and \textit{mergers} will yield the optimal integral solution. Therefore we propose the following iterative alogrithm: In the first step the problem will be solved without division and merger constraints. Then, if there are flow conservation violation, the constraints are added for the relevant nodes. The last step is repeated until either a valid solution found or no new nodes with violations are found (leaving us with an invalid solution).


\subsection{Implementation}

We built our algorithm as an extension to the \textit{Multi Hypothesis Tracking} tool by Carsten. It is implemented in C++ and the inference was performed with Gurobi (although CPLEX can also be used). The source code is available on \url{https://github.com/Ynzl/multiHypothesesTracking}.

\section{Experiments and Results}

We test our algorithm on two challenging datasets, a 3D+t drosophila scan (DRO) and 2D+t pancreatic rat stem cells (PSC) presented in Rapoport. There are two versions to both datasets, the full set including all mergers and a reduced version where mergers are omitted (DROr, PSCr). These datasets were given in the form of readily segmentated graphical models.

We track these models in five different ways. First we run the original tracking on the full model with all constraints added. We use this solution (referred to as \textit{Full}) as our baseline, \ie ground truth, to evaluate the tracking performance of the other runs. These are for one our \textit{Cutting Constraint} algorithm (CC) and also the same experiments run with LP relaxation. Lastly, a variation of \textit{Cutting Constraint} is used where the first iteration is run with LP relaxation and then the integrality constraint is turned on.

We rate two solutions by checking for the agreement of \textit{move}, \textit{merge} and \textit{division} events per pair of consecutive frames.

\subsection{Solutions}

\subsection{Computation Time}


\section{Conclusion}

It works but isn't really worth it.


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
